---
title: 'SFT vs RL'
date: 2026-02-17
permalink: /posts/sft_vs_rl/
---
## Post-Training: How RL and SFT Work Hand-in-Hand

Over the past few months, my research team and I have been working on a project to teach a large language model to play exploitative poker. More specifically, we are trying to train an LLM to use a poker solver (math-based poker software) in order to play in a way that exploits its opponents biases. I've learned a lot on this journey, and one thing that stood out was how clearly the project exposed the roles of supervised fine-tuning and reinforcement learning in modern post-training.

Very early on, it became clear that before we could optimize how well the model played poker, we first had to ensure it knew how to play at all in a usable sense. The model’s outputs needed to follow a strict, solver-compatible format. If the format was wrong, nothing downstream mattered.

This forced a useful distinction. The first phase of post-training was not about improving decision quality, but about establishing a stable and interpretable behavior. Only once that foundation was in place did it make sense to ask whether the model’s decisions were good.

This is where supervised fine-tuning (SFT) naturally comes in. SFT is fundamentally about teaching the model *how* to behave. This includes output format, reasoning style, and the overall shape of responses. After SFT, the model may not be optimal, but it is predictable. That predictability matters, because reinforcement learning (RL) only works when the behavior being optimized already exists.

> A model must first exhibit a behavior in order for us to reinforce it.

Once the model’s outputs in our poker setup were structurally correct, the problem changed. The question was no longer “is this output usable?” but “is this output good poker?” That is where RL becomes appropriate. The format stays fixed, but the reward signal pushes the model toward better decisions. How we quantify “better” is the art of designing a reward signal. In poker, for example, “better” may mean decisions that win more chips in the long run.

Stepping back, this experience reshaped how I think about post-training more generally. For a long time, I thought of post-training as a binary choice. You either use SFT to shape a model’s behavior, or you use RL to optimize it. What I’ve realized more recently is that this framing is misleading. Modern post-training works because SFT and RL sequentially build on top of each other, not because one replaces the other.

In a nutshell: 
<br>**SFT defines the behavior space, and RL optimizes within that behavior space.**

RL does not invent new behaviors from scratch. Instead, it shifts preferences among behaviors the model already knows how to produce. Essentially, it answers the following question: given multiple valid responses, which ones should the model prefer?

This distinction becomes clear when looking at how DeepSeek approached reasoning. They did not rely on RL to teach the model how to produce chains of thought. That capability was introduced through SFT, where the model learned the *form* of step-by-step reasoning. Only after that foundation was in place did RL become useful. RL was then applied to encourage chains of thought that were more accurate and aligned with the task objective.

> SFT creates a behavior → RL refines it.

Seen this way, SFT and RL are not competing approaches. They are complementary stages in a single pipeline:

- SFT makes the model legible and controllable  
- RL makes the model better at the objective we care about  

Framing post-training this way makes it easy to understand why many successful systems rely on both, and why trying to use RL without a strong SFT foundation often leads to unstable or uninterpretable behavior.


